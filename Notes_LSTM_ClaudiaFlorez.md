# Notes orales ‚Äî Les R√©seaux LSTM  
### Pr√©sentation de Claudia Florez  
_Intelligence Artificielle, 2025_  

---

## üß† Introduction
Les r√©seaux de neurones classiques traitent les donn√©es de mani√®re ind√©pendante, sans m√©moire du pass√©.  
Les **R√©seaux de Neurones R√©currents (RNN)** ont introduit la capacit√© de conserver une trace temporelle, mais ils souffrent d‚Äôun probl√®me majeur : **l‚Äôoubli du contexte √† long terme**.

> Les LSTM (Long Short-Term Memory), propos√©s par *Hochreiter et Schmidhuber (1997)*, ont √©t√© con√ßus pour surmonter cette limite.

---

## ‚ö†Ô∏è Le probl√®me des RNN classiques
Les RNN standards peuvent se souvenir de quelques pas temporels, mais pas de longues s√©quences.  
Lorsqu‚Äôune d√©pendance se trouve loin dans le temps, les gradients s‚Äôatt√©nuent ‚Äî c‚Äôest le **vanishing gradient problem**.

> Exemple : pour pr√©dire la phrase ‚ÄúJe parle fran√ßais‚Äù, le mod√®le doit se souvenir du mot ‚ÄúFrance‚Äù mentionn√© bien plus t√¥t.

Cette incapacit√© √† g√©rer les d√©pendances longues limite leur performance dans le traitement du langage, la voix ou les s√©ries temporelles.

---

## üí° L‚Äôid√©e du LSTM
Le **LSTM** introduit un **m√©canisme de m√©moire contr√¥l√©e** permettant au r√©seau d‚Äôapprendre *quoi retenir et quoi oublier*.  
Son c≈ìur est la **cellule m√©moire** (*cell state*), un flux d‚Äôinformation principal, modul√© par trois portes :

1. **Porte d‚Äôoubli** (*forget gate*) : supprime les informations inutiles.  
2. **Porte d‚Äôentr√©e** (*input gate*) : ajoute les nouvelles informations pertinentes.  
3. **Porte de sortie** (*output gate*) : d√©cide ce qui est transmis √† la sortie.

> Ces portes utilisent des fonctions sigmo√Ødes (valeurs entre 0 et 1) pour contr√¥ler le flux d‚Äôinformation.  

---

## ‚öôÔ∏è Fonctionnement √©tape par √©tape

# üßÆ Les Formules du LSTM (Long Short-Term Memory)

Le LSTM est une version am√©lior√©e du r√©seau r√©current (RNN) qui permet de **m√©moriser des informations sur de longues s√©quences**.  
Il utilise trois *portes principales* ‚Äî oubli, entr√©e et sortie ‚Äî pour g√©rer le flux d‚Äôinformations.

√Ä chaque √©tape temporelle \( t \), le mod√®le re√ßoit :
- \( x^{(t)} \) : l‚Äôentr√©e actuelle (par exemple, un mot)
- \( h^{(t-1)} \) : la sortie pr√©c√©dente (m√©moire courte)
- \( c^{(t-1)} \) : l‚Äô√©tat de la cellule pr√©c√©dente (m√©moire longue)

---

## üüß 1. Porte d‚ÄôOubli (*Forget Gate*)

D√©cide quelles informations de la m√©moire pr√©c√©dente \( c^{(t-1)} \) doivent √™tre **supprim√©es ou conserv√©es**.

\[
f^{(t)} = \sigma(W_f \cdot [h^{(t-1)}, x^{(t)}] + b_f)
\]

- \( f^{(t)} \) prend des valeurs entre 0 et 1 :  
  - 0 ‚Üí oubli total  
  - 1 ‚Üí conservation compl√®te

---

## üü© 2. Porte d‚ÄôEntr√©e (*Input Gate*)

Contr√¥le **quelle nouvelle information** doit √™tre ajout√©e √† la m√©moire.

\[
i^{(t)} = \sigma(W_i \cdot [h^{(t-1)}, x^{(t)}] + b_i)
\]
\[
\tilde{c}^{(t)} = \tanh(W_c \cdot [h^{(t-1)}, x^{(t)}] + b_c)
\]

- \( i^{(t)} \) : d√©cide combien de la nouvelle information sera int√©gr√©e  
- \( \tilde{c}^{(t)} \) : vecteur de **nouvelles valeurs candidates** √† ajouter √† la m√©moire

---

## üß± 3. Mise √† Jour de la M√©moire (*Cell State Update*)

Combine l‚Äôancienne m√©moire \( c^{(t-1)} \) et la nouvelle pour former l‚Äô√©tat actualis√© \( c^{(t)} \) :

\[
c^{(t)} = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot \tilde{c}^{(t)}
\]

o√π \( \odot \) repr√©sente la **multiplication √©l√©ment par √©l√©ment** (*Hadamard product*).

---

## üü¶ 4. Porte de Sortie (*Output Gate*)

D√©cide **quelle partie de la m√©moire** sera visible dans la sortie finale \( h^{(t)} \).

\[
o^{(t)} = \sigma(W_o \cdot [h^{(t-1)}, x^{(t)}] + b_o)
\]
\[
h^{(t)} = o^{(t)} \odot \tanh(c^{(t)})
\]

- \( o^{(t)} \) : filtre la sortie  
- \( h^{(t)} \) : sortie r√©elle du LSTM (et entr√©e du pas suiva


> Ces formules assurent que l‚Äôinformation utile est conserv√©e pendant de longues s√©quences, tout en √©vitant l‚Äôexplosion ou la disparition des gradients.

---

## üöÄ Applications et variantes

### üåç Domaines d‚Äôapplication
- Traduction automatique (Google Translate, DeepL)  
- Reconnaissance vocale (Siri, Alexa)  
- Pr√©diction de s√©ries temporelles (finance, m√©t√©o, sant√©)  
- G√©n√©ration de texte (anciens mod√®les GPT, analyse de sentiments)

### üîß Variantes du LSTM
- **Peephole LSTM** : les portes consultent l‚Äô√©tat de la cellule.  
- **GRU (Gated Recurrent Unit)** : version simplifi√©e combinant les portes d‚Äôentr√©e et d‚Äôoubli.  
- **Coupled Forget/Input Gates** : r√©duction du nombre de param√®tres.  

> Malgr√© ces variantes, les performances restent comparables (Greff et al., 2015).

---

## üìò Conclusion ‚Äî Vers les Transformers
Les **LSTM** ont marqu√© une √©tape cruciale dans le deep learning.  
Ils ont permis aux r√©seaux de **stabiliser l‚Äôapprentissage s√©quentiel** et de **m√©moriser sur le long terme**.

Aujourd‚Äôhui, les mod√®les **Transformers** ont remplac√© les LSTM dans de nombreux domaines gr√¢ce au **m√©canisme d‚Äôattention**, qui apprend les d√©pendances globales entre tous les √©l√©ments d‚Äôune s√©quence.

> Les LSTM restent n√©anmoins essentiels pour comprendre l‚Äô√©volution des architectures s√©quentielles modernes.  

---

**R√©sum√© final :**
- Les RNN = m√©moire courte.  
- Les LSTM = m√©moire longue contr√¥l√©e.  
- Les Transformers = m√©moire globale avec attention.  

‚ú® _Les LSTM sont la passerelle entre le pass√© des RNN et l‚Äôavenir des Transformers._
