# ğŸ“˜ Explication DÃ©taillÃ©e du Notebook LSTM (Claud-IA)

## ğŸ¯ Objectif du Projet
Ce notebook montre pas Ã  pas la mise en place d'un rÃ©seau de neurones **LSTM (Long Short-Term Memory)** pour l'analyse de sentiments sur le jeu de donnÃ©es **IMDb**.

---

## ğŸ”§ 1. Environnement et Versions
```python
import sys, tensorflow as tf, numpy as np, matplotlib
print("Python:", sys.version.split()[0])
print("TensorFlow:", tf.__version__)
print("NumPy:", np.__version__)
print("Matplotlib:", matplotlib.__version__)
```
VÃ©rifie la cohÃ©rence de l'environnement d'exÃ©cution.

---

## ğŸ“¦ 2. Imports et ParamÃ¨tres
Les bibliothÃ¨ques principales :
```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
```
Ces modules servent Ã  charger les donnÃ©es, prÃ©parer les sÃ©quences, construire le modÃ¨le et visualiser les performances.

---

## ğŸ—‚ï¸ 3. Chargement et PrÃ©paration du Dataset
```python
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)
X_train = pad_sequences(X_train, maxlen=200, padding='pre', truncating='pre')
X_test  = pad_sequences(X_test,  maxlen=200, padding='pre', truncating='pre')
```
Chaque critique devient une sÃ©quence de 200 indices numÃ©riques reprÃ©sentant les mots les plus frÃ©quents.

---

## ğŸ§  4. Construction du ModÃ¨le LSTM
```python
model = Sequential([
    Embedding(10000, 128, name="embedding"),
    LSTM(128, dropout=0.2, name="lstm"),
    Dense(1, activation="sigmoid", name="output")
])
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
```
- **Embedding** : transforme les indices de mots en vecteurs denses.
- **LSTM** : apprend les dÃ©pendances temporelles entre les mots.
- **Dense + SigmoÃ¯de** : sortie entre 0 et 1 (probabilitÃ© dâ€™avis positif).

> â„¹ï¸ Lâ€™argument `input_length` est facultatif â€” Keras lâ€™infÃ¨re automatiquement.

---

## ğŸƒ 5. EntraÃ®nement et Graphiques
```python
es = EarlyStopping(monitor="val_loss", patience=2, restore_best_weights=True)
history = model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=64, callbacks=[es])
```
- **EarlyStopping** : arrÃªte automatiquement lâ€™entraÃ®nement avant le surapprentissage.

### ğŸ“‰ Graphique de la perte
Visualise la diminution de lâ€™erreur pendant lâ€™entraÃ®nement.

### ğŸ“ˆ Graphique de lâ€™accuracy
Montre lâ€™Ã©volution du taux de rÃ©ussite sur lâ€™ensemble dâ€™entraÃ®nement et de validation.

---

## âœ… 6. Ã‰valuation
```python
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}")
```
Ã‰value le modÃ¨le sur des donnÃ©es jamais vues â€” typiquement **85 % de prÃ©cision**.

---

## ğŸ§ª 7. PrÃ©dictions PersonnalisÃ©es
```python
def predict_text(model, raw_text):
    tokens = [2 if word not in word_index else word_index[word]+3 for word in raw_text.lower().split()]
    seq = pad_sequences([tokens], maxlen=200)
    p = model.predict(seq, verbose=0)[0,0]
    return "Positif" if p >= 0.5 else "NÃ©gatif", p
```
Permet de tester le modÃ¨le avec ses propres phrases.

---

## ğŸ§­ 8. Conclusion
Le modÃ¨le LSTM est capable de comprendre le **sentiment gÃ©nÃ©ral** dâ€™un texte Ã  partir de sa structure sÃ©quentielle.  
Câ€™est une architecture clÃ© du deep learning pour le traitement du langage naturel.

---

### ğŸ’¬ RÃ©sumÃ© Oral
> Â« Le LSTM apprend Ã  mÃ©moriser les informations utiles dâ€™un texte tout en oubliant le reste.  
> GrÃ¢ce Ã  son mÃ©canisme de portes, il parvient Ã  comprendre les Ã©motions exprimÃ©es dans les phrases. Â»

---

## ğŸ” Pour Aller Plus Loin â€” AmÃ©liorations du ModÃ¨le LSTM

### ğŸ§­ 1. Bidirectional(LSTM)
- **Principe :** lit la sÃ©quence dans les deux sens (gauche â†’ droite et droite â†’ gauche).  
- **Avantage :** comprend le contexte complet autour de chaque mot.  
- **Code :**
```python
from tensorflow.keras.layers import Bidirectional, LSTM
model.add(Bidirectional(LSTM(128, dropout=0.2)))
```
- **Effet :** +2 Ã  +5 % dâ€™accuracy sur les tÃ¢ches textuelles.

### ğŸ§  2. Embeddings prÃ©-entraÃ®nÃ©s (GloVe, Word2Vec)
- **Principe :** utiliser des vecteurs de mots dÃ©jÃ  appris sur de grands corpus (Wikipedia, Google Newsâ€¦).  
- **Avantage :** meilleure comprÃ©hension sÃ©mantique sans nÃ©cessiter beaucoup de donnÃ©es.  
- **Exemples :** GloVe, Word2Vec, FastText.  
- **Effet :** amÃ©liore la prÃ©cision et la gÃ©nÃ©ralisation du modÃ¨le.

### ğŸ§± 3. RÃ©gularisation (Dropout, EarlyStopping, ReduceLROnPlateau)
- **Dropout :** dÃ©sactive alÃ©atoirement des neurones pour Ã©viter le surapprentissage.  
  Exemple : `LSTM(128, dropout=0.3)` coupe 30 % des connexions.  
- **EarlyStopping :** arrÃªte l'entraÃ®nement quand la validation n'amÃ©liore plus.  
- **ReduceLROnPlateau :** diminue automatiquement le *learning rate* quand la perte stagne.  
- **Effet :** stabilise le modÃ¨le et amÃ©liore la convergence.

### âœï¸ 4. Nettoyage NLP plus riche
| Ã‰tape | Action | Exemple |
|-------|---------|----------|
| **Lowercasing** | Mettre tout en minuscules | â€œFilmâ€ â†’ â€œfilmâ€ |
| **Suppression de ponctuation** | Enlever â€œ!â€, â€œ?â€, â€œ.â€ | â€œGÃ©nial!â€ â†’ â€œgÃ©nialâ€ |
| **Stopwords** | Retirer les mots sans signification | â€œleâ€, â€œdeâ€, â€œetâ€â€¦ |
| **Lemmatisation** | Ramener Ã  la racine | â€œjouaitâ€ â†’ â€œjouerâ€ |
| **Tokenisation** | DÃ©couper le texte en mots | â€œce film est bienâ€ â†’ [â€œceâ€, â€œfilmâ€, â€œestâ€, â€œbienâ€] |

> ğŸ™ï¸ Ã€ dire : Â« Le nettoyage du texte est essentiel pour rÃ©duire le bruit et aider le LSTM Ã  se concentrer sur le sens. Â»

### ğŸ” 5. CohÃ©rence et ReproductibilitÃ©
- **CohÃ©rence train/val/test :** appliquer exactement les mÃªmes transformations aux trois ensembles.  
- **ReproductibilitÃ© :** fixer les graines alÃ©atoires et sauvegarder le modÃ¨le pour garantir les mÃªmes rÃ©sultats Ã  chaque exÃ©cution.  
- **Effet :** un entraÃ®nement fiable et vÃ©rifiable.

> ğŸ’¬ *Ã€ retenir :* Ces techniques rendent le modÃ¨le plus robuste, plus cohÃ©rent et plus professionnel. Elles constituent les bases dâ€™un pipeline NLP avancÃ©.
