# ğŸ“˜ Explication DÃ©taillÃ©e du Notebook LSTM (Claud-IA)

## ğŸ¯ Objectif du Projet
Ce notebook montre pas Ã  pas la mise en place d'un rÃ©seau de neurones **LSTM (Long Short-Term Memory)** pour l'analyse de sentiments sur le jeu de donnÃ©es **IMDb**.

---

## ğŸ”§ 1. Environnement et Versions
```python
import sys, tensorflow as tf, numpy as np, matplotlib
print("Python:", sys.version.split()[0])
print("TensorFlow:", tf.__version__)
print("NumPy:", np.__version__)
print("Matplotlib:", matplotlib.__version__)
```
VÃ©rifie la cohÃ©rence de l'environnement d'exÃ©cution.

---

## ğŸ“¦ 2. Imports et ParamÃ¨tres
Les bibliothÃ¨ques principales :
```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
```
Ces modules servent Ã  charger les donnÃ©es, prÃ©parer les sÃ©quences, construire le modÃ¨le et visualiser les performances.

---

## ğŸ—‚ï¸ 3. Chargement et PrÃ©paration du Dataset
```python
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)
X_train = pad_sequences(X_train, maxlen=200, padding='pre', truncating='pre')
X_test  = pad_sequences(X_test,  maxlen=200, padding='pre', truncating='pre')
```
Chaque critique devient une sÃ©quence de 200 indices numÃ©riques reprÃ©sentant les mots les plus frÃ©quents.

---

## ğŸ§  4. Construction du ModÃ¨le LSTM
```python
model = Sequential([
    Embedding(10000, 128, name="embedding"),
    LSTM(128, dropout=0.2, name="lstm"),
    Dense(1, activation="sigmoid", name="output")
])
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
```
- **Embedding** : transforme les indices de mots en vecteurs denses.
- **LSTM** : apprend les dÃ©pendances temporelles entre les mots.
- **Dense + SigmoÃ¯de** : sortie entre 0 et 1 (probabilitÃ© dâ€™avis positif).

> â„¹ï¸ Lâ€™argument `input_length` est facultatif â€” Keras lâ€™infÃ¨re automatiquement.

---

## ğŸƒ 5. EntraÃ®nement et Graphiques
```python
es = EarlyStopping(monitor="val_loss", patience=2, restore_best_weights=True)
history = model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=64, callbacks=[es])
```
- **EarlyStopping** : arrÃªte automatiquement lâ€™entraÃ®nement avant le surapprentissage.

### ğŸ“‰ Graphique de la perte
Visualise la diminution de lâ€™erreur pendant lâ€™entraÃ®nement.

### ğŸ“ˆ Graphique de lâ€™accuracy
Montre lâ€™Ã©volution du taux de rÃ©ussite sur lâ€™ensemble dâ€™entraÃ®nement et de validation.

---

## âœ… 6. Ã‰valuation
```python
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}")
```
Ã‰value le modÃ¨le sur des donnÃ©es jamais vues â€” typiquement **85 % de prÃ©cision**.

---

## ğŸ§ª 7. PrÃ©dictions PersonnalisÃ©es
```python
def predict_text(model, raw_text):
    tokens = [2 if word not in word_index else word_index[word]+3 for word in raw_text.lower().split()]
    seq = pad_sequences([tokens], maxlen=200)
    p = model.predict(seq, verbose=0)[0,0]
    return "Positif" if p >= 0.5 else "NÃ©gatif", p
```
Permet de tester le modÃ¨le avec ses propres phrases.

---

## ğŸ§­ 8. Conclusion
Le modÃ¨le LSTM est capable de comprendre le **sentiment gÃ©nÃ©ral** dâ€™un texte Ã  partir de sa structure sÃ©quentielle.  
Câ€™est une architecture clÃ© du deep learning pour le traitement du langage naturel.

---

### ğŸ’¬ RÃ©sumÃ© Oral
> Â« Le LSTM apprend Ã  mÃ©moriser les informations utiles dâ€™un texte tout en oubliant le reste.  
> GrÃ¢ce Ã  son mÃ©canisme de portes, il parvient Ã  comprendre les Ã©motions exprimÃ©es dans les phrases. Â»
