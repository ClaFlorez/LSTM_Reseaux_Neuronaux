# Notes orales ‚Äî Les R√©seaux LSTM  
### Pr√©sentation de Claudia Florez  
_Intelligence Artificielle, 2025_  

---

## üß† Introduction
Les r√©seaux de neurones classiques traitent les donn√©es de mani√®re ind√©pendante, sans m√©moire du pass√©.  
Les **R√©seaux de Neurones R√©currents (RNN)** ont introduit la capacit√© de conserver une trace temporelle, mais ils souffrent d‚Äôun probl√®me majeur : **l‚Äôoubli du contexte √† long terme**.

> Les LSTM (Long Short-Term Memory), propos√©s par *Hochreiter et Schmidhuber (1997)*, ont √©t√© con√ßus pour surmonter cette limite.

---

## ‚ö†Ô∏è Le probl√®me des RNN classiques
Les RNN standards peuvent se souvenir de quelques pas temporels, mais pas de longues s√©quences.  
Lorsqu‚Äôune d√©pendance se trouve loin dans le temps, les gradients s‚Äôatt√©nuent ‚Äî c‚Äôest le **vanishing gradient problem**.

> Exemple : pour pr√©dire la phrase ‚ÄúJe parle fran√ßais‚Äù, le mod√®le doit se souvenir du mot ‚ÄúFrance‚Äù mentionn√© bien plus t√¥t.

Cette incapacit√© √† g√©rer les d√©pendances longues limite leur performance dans le traitement du langage, la voix ou les s√©ries temporelles.

---

## üí° L‚Äôid√©e du LSTM
Le **LSTM** introduit un **m√©canisme de m√©moire contr√¥l√©e** permettant au r√©seau d‚Äôapprendre *quoi retenir et quoi oublier*.  
Son c≈ìur est la **cellule m√©moire** (*cell state*), un flux d‚Äôinformation principal, modul√© par trois portes :

1. **Porte d‚Äôoubli** (*forget gate*) : supprime les informations inutiles.  
2. **Porte d‚Äôentr√©e** (*input gate*) : ajoute les nouvelles informations pertinentes.  
3. **Porte de sortie** (*output gate*) : d√©cide ce qui est transmis √† la sortie.

> Ces portes utilisent des fonctions sigmo√Ødes (valeurs entre 0 et 1) pour contr√¥ler le flux d‚Äôinformation.  

---

## ‚öôÔ∏è Fonctionnement √©tape par √©tape

### üîπ 1. Porte d‚Äôoubli
La porte d‚Äôoubli choisit quelles parties de la m√©moire pass√©e \(C_{t-1}\) doivent √™tre effac√©es :  
$$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$

### üîπ 2. Porte d‚Äôentr√©e
Elle d√©termine quelles nouvelles informations \(\tilde{C}_t\) seront ajout√©es :  
$$ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) $$  
$$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) $$

### üîπ 3. Mise √† jour de la m√©moire
La cellule m√©moire est mise √† jour selon :  
$$ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t $$

### üîπ 4. Porte de sortie et √©tat cach√©
Enfin, la sortie est calcul√©e :  
$$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$  
$$ h_t = o_t * \tanh(C_t) $$

> Ces formules assurent que l‚Äôinformation utile est conserv√©e pendant de longues s√©quences, tout en √©vitant l‚Äôexplosion ou la disparition des gradients.

---

## üöÄ Applications et variantes

### üåç Domaines d‚Äôapplication
- Traduction automatique (Google Translate, DeepL)  
- Reconnaissance vocale (Siri, Alexa)  
- Pr√©diction de s√©ries temporelles (finance, m√©t√©o, sant√©)  
- G√©n√©ration de texte (anciens mod√®les GPT, analyse de sentiments)

### üîß Variantes du LSTM
- **Peephole LSTM** : les portes consultent l‚Äô√©tat de la cellule.  
- **GRU (Gated Recurrent Unit)** : version simplifi√©e combinant les portes d‚Äôentr√©e et d‚Äôoubli.  
- **Coupled Forget/Input Gates** : r√©duction du nombre de param√®tres.  

> Malgr√© ces variantes, les performances restent comparables (Greff et al., 2015).

---

## üìò Conclusion ‚Äî Vers les Transformers
Les **LSTM** ont marqu√© une √©tape cruciale dans le deep learning.  
Ils ont permis aux r√©seaux de **stabiliser l‚Äôapprentissage s√©quentiel** et de **m√©moriser sur le long terme**.

Aujourd‚Äôhui, les mod√®les **Transformers** ont remplac√© les LSTM dans de nombreux domaines gr√¢ce au **m√©canisme d‚Äôattention**, qui apprend les d√©pendances globales entre tous les √©l√©ments d‚Äôune s√©quence.

> Les LSTM restent n√©anmoins essentiels pour comprendre l‚Äô√©volution des architectures s√©quentielles modernes.  

---

**R√©sum√© final :**
- Les RNN = m√©moire courte.  
- Les LSTM = m√©moire longue contr√¥l√©e.  
- Les Transformers = m√©moire globale avec attention.  

‚ú® _Les LSTM sont la passerelle entre le pass√© des RNN et l‚Äôavenir des Transformers._
